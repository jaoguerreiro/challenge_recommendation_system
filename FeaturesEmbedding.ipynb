{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReuYQdnXswi6",
        "outputId": "2762b3dc-192e-4a22-fbfa-ec4a0f2c530d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "504ecea9d77b03c"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/x_train_Meacfjr.csv\", delimiter=\",\", quotechar='\"')\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/y_train_SwJNMSu.csv\", delimiter=\",\", quotechar='\"')\n",
        "\n",
        "# Convert 'job_ids' and 'actions' from strings to actual lists\n",
        "X_train[\"job_ids\"] = X_train[\"job_ids\"].apply(ast.literal_eval)\n",
        "X_train[\"actions\"] = X_train[\"actions\"].apply(ast.literal_eval)\n",
        "\n",
        "df_jobs = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/job_features.csv\")\n",
        "\n",
        "df_job_location = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/job_features_location.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rJbNHuQlPBj",
        "outputId": "ebc644bc-55e8-4730-afb8-9684829348e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21917 entries, 0 to 21916\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   job_id           21917 non-null  int64 \n",
            " 1   title_section    21913 non-null  object\n",
            " 2   job_description  21917 non-null  object\n",
            " 3   seniority        21897 non-null  object\n",
            " 4   company          21917 non-null  object\n",
            " 5   industry         21905 non-null  object\n",
            " 6   location         21916 non-null  object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 1.2+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "for _, row in df_job_location.iterrows():\n",
        "    df_jobs.loc[df_jobs['job_id'] == row['job_id'], 'location'] = row['location']\n",
        "\n",
        "print(df_jobs.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvFXy_v2hxRi"
      },
      "source": [
        "## GPU Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj8FZCXshH-Y",
        "outputId": "19c298f2-e161-4780-b37e-b0fb06bdfb3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version # find the CUDA driver build above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6MNYPcr9G0A",
        "outputId": "1f860266-731b-4bd1-f7c9-9be36b6e16f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 11 14:14:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owb5PR9L9IF8",
        "outputId": "c4fa8f47-4ef2-48e3-a010-0e820a02da2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fad165e551f6ea0"
      },
      "source": [
        "## 1.3 Embedding of the Job Ads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e445478a03eaa9ec"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NvxFy94nxvBN"
      },
      "outputs": [],
      "source": [
        "# Define features and weights\n",
        "features = ['title_section', 'location', 'seniority', 'company', 'industry']  # Example with multiple features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5c84f91b9f7004b7"
      },
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained sentence transformer model and move it to the GPU if available\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2').to(device)  # intfloat/multilingual-e5-large\n",
        "\n",
        "def compute_job_embeddings(df, features, embedding_dim=384):  # Adjust `embedding_dim` to match your model\n",
        "    job_embeddings = {feature: [] for feature in features}  # Store embeddings per feature\n",
        "    mask_vectors = {feature: [] for feature in features}  # Store masks (1 for valid, 0 for missing)\n",
        "\n",
        "    # Compute all embeddings\n",
        "    for idx, row in df.iterrows():\n",
        "        if idx % 1000 == 0:\n",
        "            print(idx)\n",
        "        for feature in features:\n",
        "            if pd.notna(row[feature]) and row[feature] is not None and len(row[feature]) > 1:  # If feature is not missing\n",
        "                feature_vector = model.encode(str(row[feature]), convert_to_tensor=True, show_progress_bar=False).to(device)\n",
        "                mask_vectors[feature].append(1)  # Mark as valid\n",
        "            else:  # If feature is missing\n",
        "                feature_vector = torch.zeros(embedding_dim, device=device)  # Assign zero vector\n",
        "                mask_vectors[feature].append(0)  # Mark as missing\n",
        "\n",
        "            job_embeddings[feature].append(feature_vector)\n",
        "\n",
        "    # Convert to PyTorch tensors for each feature\n",
        "    for feature in features:\n",
        "        job_embeddings[feature] = torch.stack(job_embeddings[feature])\n",
        "        mask_vectors[feature] = torch.tensor(mask_vectors[feature], dtype=torch.float32, device=device).view(-1, 1)\n",
        "\n",
        "    # Zero-mean normalization only for valid embeddings\n",
        "    for feature in features:\n",
        "        valid_embeddings = job_embeddings[feature] * mask_vectors[feature]  # Apply mask\n",
        "        mean = valid_embeddings.sum(dim=0) / mask_vectors[feature].sum()  # Compute mean ignoring missing rows\n",
        "        job_embeddings[feature] = (job_embeddings[feature] - mean) * mask_vectors[feature]  # Normalize only valid rows\n",
        "\n",
        "    return job_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "154391116910a516",
        "collapsed": true,
        "outputId": "5bfeedcf-05dc-4a67-90e1-6628bbe174fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings from /content/drive/MyDrive/Colab Notebooks/Challenge/data/job_embeddings.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-db80a4d4e358>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  job_embeddings = torch.load(embedding_path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "# Load or Compute Embeddings\n",
        "embedding_path = \"/content/drive/MyDrive/Colab Notebooks/Challenge/data/job_embeddings.pth\"\n",
        "if os.path.exists(embedding_path):\n",
        "    print(\"Loading embeddings from {}\".format(embedding_path))\n",
        "    job_embeddings = torch.load(embedding_path, map_location=device)\n",
        "else:\n",
        "    print(\"Computing embeddings...\")\n",
        "    job_embeddings = compute_job_embeddings(df_jobs, features)\n",
        "    torch.save(job_embeddings, embedding_path)\n",
        "\n",
        "# Add embeddings to DataFrame\n",
        "for feature in features:\n",
        "    df_jobs[f'embedding_{feature}'] = list(job_embeddings[feature].cpu())\n",
        "\n",
        "# Create a mapping from job_id to its embeddings (per feature)\n",
        "job_embedding_dict = {row['job_id']: {feature: row[f'embedding_{feature}'] for feature in features}\n",
        "                      for _, row in df_jobs.iterrows()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_embeddings(df_jobs, job_embedding_dict):\n",
        "    \"\"\" Convert job embeddings into a single tensor for fast access. \"\"\"\n",
        "    job_id_to_idx = {job_id: idx for idx, job_id in enumerate(df_jobs['job_id'].values)}\n",
        "\n",
        "    job_embeddings_tensor = torch.stack([\n",
        "        torch.stack([job_embedding_dict[job_id][feature] * feature_weights[feature] for feature in features]).sum(dim=0)\n",
        "        for job_id in df_jobs['job_id']\n",
        "    ]).to(torch.float32).to('cuda')  # Shape: (num_jobs, embedding_dim)\n",
        "\n",
        "    job_ids_tensor = torch.tensor(df_jobs['job_id'].values, dtype=torch.long, device='cuda')  # (num_jobs,)\n",
        "\n",
        "    return job_embeddings_tensor, job_ids_tensor, job_id_to_idx\n",
        "\n",
        "def preprocess_sessions(x, job_id_to_idx, action_weights):\n",
        "    \"\"\" Convert session interactions into tensor format for batch processing. \"\"\"\n",
        "    max_jobs_per_session = max(len(job_ids) for job_ids in x['job_ids'])\n",
        "\n",
        "    session_ids = torch.tensor(x['session_id'].values, dtype=torch.long, device='cuda')\n",
        "\n",
        "    job_indices = torch.full((len(x), max_jobs_per_session), -1, dtype=torch.long, device='cuda')\n",
        "    action_weights_tensor = torch.zeros((len(x), max_jobs_per_session), dtype=torch.float32, device='cuda')\n",
        "\n",
        "    for i, (job_ids, actions) in enumerate(zip(x['job_ids'], x['actions'])):\n",
        "        valid_indices = [job_id_to_idx[jid] for jid in job_ids if jid in job_id_to_idx]\n",
        "        job_indices[i, :len(valid_indices)] = torch.tensor(valid_indices, dtype=torch.long, device='cuda')\n",
        "        action_weights_tensor[i, :len(valid_indices)] = torch.tensor(\n",
        "            [action_weights[a] for a in actions[:len(valid_indices)]], dtype=torch.float32, device='cuda'\n",
        "        )\n",
        "\n",
        "    return session_ids, job_indices, action_weights_tensor\n"
      ],
      "metadata": {
        "id": "lZ87iA77BOQ_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_mrr(y_true, y_pred, verbose=True):\n",
        "    \"\"\"\n",
        "    Computes Mean Reciprocal Rank (MRR).\n",
        "\n",
        "    Args:\n",
        "        y_true: dataframe of true job_id values per session.\n",
        "        y_pred: Predicted action and 10 most relevant job_ids per session.\n",
        "\n",
        "    Returns:\n",
        "        MRR final score (float).\n",
        "    \"\"\"\n",
        "    y_true_jobs = y_true[\"job_id\"]\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for true_job, (action, predicted_jobs) in zip(y_true_jobs, y_pred):\n",
        "        try:\n",
        "            rank = predicted_jobs.index(true_job) + 1  # Get 1-based rank\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "        except ValueError:\n",
        "            reciprocal_ranks.append(0)  # y_true not found in top-10\n",
        "\n",
        "    reciprocal_ranks_tensor = torch.tensor(reciprocal_ranks, dtype=torch.float32)\n",
        "\n",
        "    # Count occurrences using torch (no Counter)\n",
        "    unique_values, counts = torch.unique(reciprocal_ranks_tensor, return_counts=True)\n",
        "\n",
        "    if verbose:\n",
        "        # Display value counts\n",
        "        print(\"\\nReciprocal Rank Value Counts:\")\n",
        "        for val, count in zip(unique_values.tolist(), counts.tolist()):\n",
        "            print(f\"  {val:.4f}: {count} occurrences\")\n",
        "        print(\"\\nMRR:\", reciprocal_ranks_tensor.mean().item())\n",
        "\n",
        "    return reciprocal_ranks_tensor.mean().item()"
      ],
      "metadata": {
        "id": "cKtK_ds3HQxt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommend list of the 10 most relevant jobs"
      ],
      "metadata": {
        "id": "K5BlvwaSMsdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_weights = {\n",
        "    'title_section': 1.0,\n",
        "    'location': 0.8,\n",
        "    'seniority': 0.8,\n",
        "    'company': 0.5,\n",
        "    'industry': 0.5\n",
        "    }\n",
        "\n",
        "action_weights = {'apply': 1, 'view': 1}"
      ],
      "metadata": {
        "id": "IZhcxk8GMaAE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "afbe839398fe0a71"
      },
      "outputs": [],
      "source": [
        "# Function to generate recommendations for sessions dataframe using GPU\n",
        "def recommend_jobs_for_sessions(x, df_jobs, job_embedding_dict, feature_weights, action_weights, top_n=10, batch_size=64):\n",
        "    \"\"\" Fully parallelized job recommendation system using PyTorch tensor operations. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Precompute job embeddings and indices for fast access\n",
        "    job_embeddings_tensor, job_ids_tensor, job_id_to_idx = preprocess_embeddings(df_jobs, job_embedding_dict)\n",
        "\n",
        "    # Preprocess session interactions\n",
        "    session_ids, job_indices, action_weights_tensor = preprocess_sessions(x, job_id_to_idx, action_weights)\n",
        "\n",
        "    all_recommendations = []\n",
        "\n",
        "    for batch_start in range(0, len(x), batch_size):\n",
        "        if (batch_start // batch_size + 1) % 20 == 0:\n",
        "            print(\"Processing batch\", batch_start // batch_size + 1, \"of\", len(x) // batch_size + 1)\n",
        "        start_time = time.time()\n",
        "        batch_end = min(batch_start + batch_size, len(x))\n",
        "        batch_job_indices = job_indices[batch_start:batch_end]\n",
        "        batch_action_weights = action_weights_tensor[batch_start:batch_end]\n",
        "\n",
        "        # Mask out invalid job indices (-1)\n",
        "        valid_mask = batch_job_indices != -1\n",
        "        batch_job_embeddings = job_embeddings_tensor[batch_job_indices.clamp(min=0)] * valid_mask.unsqueeze(-1)\n",
        "\n",
        "        # Compute user profiles in parallel\n",
        "        batch_weighted_sums = (batch_job_embeddings * batch_action_weights.unsqueeze(-1)).sum(dim=1)\n",
        "        batch_action_sums = batch_action_weights.sum(dim=1, keepdim=True).clamp(min=1e-6)\n",
        "        batch_user_profiles = batch_weighted_sums / batch_action_sums  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Compute cosine similarity in parallel\n",
        "        similarities = F.cosine_similarity(batch_user_profiles.unsqueeze(1), job_embeddings_tensor.unsqueeze(0), dim=-1)\n",
        "\n",
        "        # Exclude jobs already interacted with\n",
        "        batch_seen_jobs = (batch_job_indices.unsqueeze(-1) == job_ids_tensor).any(dim=1)\n",
        "        similarities[batch_seen_jobs] = -float('inf')  # Mask seen jobs\n",
        "\n",
        "        # Get top-N recommendations\n",
        "        top_recommendations = similarities.topk(top_n, dim=-1).indices  # (batch_size, top_n)\n",
        "\n",
        "        # Convert to list format\n",
        "        for i in range(batch_start, batch_end):\n",
        "            session_id = session_ids[i].item()\n",
        "            recommended_job_ids = job_ids_tensor[top_recommendations[i - batch_start]].tolist()\n",
        "            all_recommendations.append((session_id, recommended_job_ids))\n",
        "\n",
        "        del batch_user_profiles, similarities, batch_seen_jobs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return all_recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3e96b8d7b04f0b",
        "outputId": "9251e4f4-10ef-485e-a609-b48b99e59461",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20 of 249\n",
            "Processing batch 40 of 249\n",
            "Processing batch 60 of 249\n",
            "Processing batch 80 of 249\n",
            "Processing batch 100 of 249\n",
            "Processing batch 120 of 249\n",
            "Processing batch 140 of 249\n",
            "Processing batch 160 of 249\n",
            "Processing batch 180 of 249\n",
            "Processing batch 200 of 249\n",
            "Processing batch 220 of 249\n",
            "Processing batch 240 of 249\n"
          ]
        }
      ],
      "source": [
        "x, y = X_train, y_train\n",
        "y_pred = recommend_jobs_for_sessions(x, df_jobs, job_embedding_dict, feature_weights, action_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_mrr(y, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjnag8dlFqGp",
        "outputId": "5a815ff7-8d28-43f1-c03f-d268677b5a7f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reciprocal Rank Value Counts:\n",
            "  0.0000: 15319 occurrences\n",
            "  0.1000: 55 occurrences\n",
            "  0.1111: 56 occurrences\n",
            "  0.1250: 70 occurrences\n",
            "  0.1429: 40 occurrences\n",
            "  0.1667: 45 occurrences\n",
            "  0.2000: 58 occurrences\n",
            "  0.2500: 50 occurrences\n",
            "  0.3333: 79 occurrences\n",
            "  0.5000: 63 occurrences\n",
            "  1.0000: 47 occurrences\n",
            "\n",
            "MRR: 0.01023925468325615\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01023925468325615"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RWu_TGINXa6"
      },
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4d2bb0844b5f595c"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}