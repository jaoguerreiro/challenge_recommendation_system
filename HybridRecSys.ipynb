{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glTPXbH2qfLU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTRUfiOHxpX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb621d7-bfe4-4bf4-90b7-7bec93dc4ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-de8b1349da62>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  job_embeddings = torch.load('/content/job_embeddings_small_norm.pth', map_location=device)\n"
          ]
        }
      ],
      "source": [
        "# Load the JSON file\n",
        "with open(\"/content/job_listings.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    job_data = json.load(f)\n",
        "\n",
        "# Import Data\n",
        "df_x = pd.read_csv('/content/x_train_Meacfjr.csv')\n",
        "df_y = pd.read_csv('/content/y_train_SwJNMSu.csv')\n",
        "\n",
        "# Job embeddings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "job_embeddings = torch.load('/content/job_embeddings_small_norm.pth', map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtskF0-Qxqo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b852fd98-3bbf-4d58-8468-0ec343c80d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading embeddings from /content/drive/MyDrive/Colab Notebooks/Challenge/data/job_embeddings_small.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-766ebfb8b1a0>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  job_embeddings = torch.load(embedding_path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # # Load the JSON file\n",
        "# with open(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/job_listings.json\", \"r\") as file:\n",
        "#     job_data = json.load(file)\n",
        "\n",
        "\n",
        "# df_x = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/x_train_Meacfjr.csv\", delimiter=\",\", quotechar='\"')\n",
        "# df_y = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Challenge/data/y_train_SwJNMSu.csv\", delimiter=\",\", quotechar='\"')\n",
        "\n",
        "# # Job Embeddings\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# embedding_path = \"/content/drive/MyDrive/Colab Notebooks/Challenge/data/job_embeddings_small.pth\"\n",
        "# print(\"Loading embeddings from {}\".format(embedding_path))\n",
        "# job_embeddings = torch.load(embedding_path, map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ3W44eXqfLV"
      },
      "outputs": [],
      "source": [
        "# Check the first few keys\n",
        "# print(list(job_data.keys())[:5])\n",
        "\n",
        "# Print a sample entry\n",
        "# sample_key = list(job_data.keys())[0]\n",
        "# print(job_data[sample_key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prl4Z3zBqfLW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 0. Load Data\n",
        "# ============================================================\n",
        "\n",
        "# Convert string representation of lists into actual lists\n",
        "df_x[\"job_ids\"] = df_x[\"job_ids\"].apply(ast.literal_eval)\n",
        "df_x[\"actions\"] = df_x[\"actions\"].apply(ast.literal_eval)\n",
        "\n",
        "#print df\n",
        "# print(df_x)\n",
        "# print(df_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpbUiDPeqfLW"
      },
      "outputs": [],
      "source": [
        "def expand_dataset(x, y):\n",
        "    y_indexed = y.set_index('session_id')\n",
        "\n",
        "    new_df_x_rows = []\n",
        "    new_df_y_rows = []\n",
        "    new_session_id = 0\n",
        "\n",
        "    for _, row in x.iterrows():\n",
        "        orig_session = row['session_id']\n",
        "        job_ids = row['job_ids']\n",
        "        actions = row['actions']\n",
        "\n",
        "        # Preserve numerical features from the original row\n",
        "        total = row.get('total', 0)  # Ensure default value if missing\n",
        "        apply_count = row.get('apply_count', 0)\n",
        "        view_count = row.get('view_count', 0)\n",
        "\n",
        "        # Get target data\n",
        "        target_row = y_indexed.loc[orig_session]\n",
        "        target_job = target_row['job_id']\n",
        "        target_action = target_row['action']\n",
        "\n",
        "        n = len(job_ids)\n",
        "        for i in range(n):\n",
        "            prefix_jobs = job_ids[:i+1]\n",
        "            prefix_actions = actions[:i+1]\n",
        "\n",
        "            if i < n - 1:\n",
        "                t_job = job_ids[i+1]\n",
        "                t_action = actions[i+1]\n",
        "            else:\n",
        "                t_job = target_job\n",
        "                t_action = target_action\n",
        "\n",
        "            new_df_x_rows.append({\n",
        "                'session_id': new_session_id,\n",
        "                'job_ids': prefix_jobs,\n",
        "                'actions': prefix_actions,\n",
        "                'total': total,  # Keep original features\n",
        "                'apply_count': apply_count,\n",
        "                'view_count': view_count\n",
        "            })\n",
        "            new_df_y_rows.append({\n",
        "                'session_id': new_session_id,\n",
        "                'job_id': t_job,\n",
        "                'action': t_action\n",
        "            })\n",
        "            new_session_id += 1\n",
        "\n",
        "    return pd.DataFrame(new_df_x_rows), pd.DataFrame(new_df_y_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model for feature weights learning"
      ],
      "metadata": {
        "id": "nsGwt1wjE4mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class JobRecommendationModel(nn.Module):\n",
        "    def __init__(self, job_embeddings, embedding_dim, device):\n",
        "        super().__init__()\n",
        "        self.job_embeddings = {k: v.to(device) for k, v in job_embeddings.items()}  # Move embeddings to GPU\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.feature_weights = nn.Parameter(torch.ones(len(job_embeddings)).to(device))  # Learnable weights\n",
        "\n",
        "    def compute_user_preference(self, job_ids):\n",
        "        \"\"\"Computes weighted user preference embedding.\"\"\"\n",
        "        job_embeds = []\n",
        "        for feature, embeddings in self.job_embeddings.items():\n",
        "            job_embeds.append(embeddings[job_ids])  # Get embeddings for the given jobs\n",
        "        job_embeds = torch.stack(job_embeds, dim=0)  # Shape: (num_features, num_jobs, embedding_dim)\n",
        "        weighted_embeds = self.feature_weights.view(-1, 1, 1) * job_embeds  # Apply weights\n",
        "        user_pref = weighted_embeds.mean(dim=1).sum(dim=0)  # Mean over jobs, sum over features\n",
        "        return user_pref / self.feature_weights.sum()  # Normalize\n",
        "\n",
        "    def forward(self, user_jobs, positive_job, negative_job):\n",
        "        \"\"\"Computes BPR loss for training.\"\"\"\n",
        "        user_pref = self.compute_user_preference(user_jobs)  # User embedding\n",
        "        # Ensure all embeddings are 2D (batch_size=1)\n",
        "        user_pref = user_pref.unsqueeze(0)  # Shape: [1, embedding_dim]\n",
        "        pos_emb = pos_emb.unsqueeze(0)      # Shape: [1, embedding_dim]\n",
        "        neg_emb = neg_emb.unsqueeze(0)      # Shape: [1, embedding_dim]\n",
        "\n",
        "        print(user_pref.shape)\n",
        "        print(self.compute_user_preference(positive_job).shape)\n",
        "        pos_sim = torch.cosine_similarity(user_pref, self.compute_user_preference(positive_job))\n",
        "        neg_sim = torch.cosine_similarity(user_pref, self.compute_user_preference(negative_job))\n",
        "        return -torch.log(torch.sigmoid(pos_sim - neg_sim)).mean()  # BPR loss"
      ],
      "metadata": {
        "id": "vsUKzA3fE8WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(job_embeddings, x_train, y_train, num_epochs=10, lr=0.01, device='cuda'):\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    model = JobRecommendationModel(job_embeddings, embedding_dim=job_embeddings[list(job_embeddings.keys())[0]].shape[1], device=device).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for _, session in x_train.groupby('session_id'):\n",
        "            job_ids = torch.tensor(session['job_ids'].values[0], dtype=torch.long, device=device)\n",
        "            positive_job = torch.tensor(y_train[y_train['session_id'] == session['session_id'].iloc[0]]['job_id'].values, dtype=torch.long, device=device)\n",
        "            negative_job = torch.randint(0, job_embeddings[list(job_embeddings.keys())[0]].shape[0], (positive_job.shape[0],), device=device)  # Random negatives\n",
        "\n",
        "            loss = model(job_ids, positive_job, negative_job)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(x_train)}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "kawjU3PCFGEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(job_embeddings, df_x, df_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "QX9ESNx5E-XJ",
        "outputId": "65e5a85c-e335-4708-9ee2-a0f47f7bf302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'pos_emb' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c1c92761442>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-58d8e7320c93>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(job_embeddings, x_train, y_train, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnegative_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Random negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0f01f0d5c189>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, user_jobs, positive_job, negative_job)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Ensure all embeddings are 2D (batch_size=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0muser_pref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_pref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [1, embedding_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Shape: [1, embedding_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mneg_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Shape: [1, embedding_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'pos_emb' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ack4YiHiz8bC"
      },
      "source": [
        "## First Version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imuzi-XT0JC8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKD6m_YU0LI8"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 1. Load Job Embeddings (Using GPU)\n",
        "# ---------------------------\n",
        "feature_weights = {\n",
        "    'title_section': 1.0,\n",
        "    'location': 0.8,\n",
        "    'seniority': 0.8,\n",
        "    'company': 0.5,\n",
        "    'industry': 0.5\n",
        "    }\n",
        "action_weights = {'apply': 10, 'view': 1}\n",
        "\n",
        "weighted_features = [feature_weights[feature] * job_embeddings[feature] for feature in feature_weights.keys()]\n",
        "job_embedding_matrix = torch.cat(weighted_features, dim=1).to(device) # job_embedding_matrix.shape = (n_jobs, n_features*emb_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HD9o-E5WE1vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_embeddings['title_section'].shape)\n",
        "print(len(weighted_features))\n",
        "print(job_embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5CX2iksBWNm",
        "outputId": "0d943772-ca5b-4773-9315-6cfce9ab4f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21917, 1024])\n",
            "5\n",
            "torch.Size([21917, 5120])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RACLmhUY0OTa"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 2. Build Interaction Matrix from df_x\n",
        "# ---------------------------\n",
        "def build_interaction_matrix(x_train, weight_map={'apply': 5, 'view': 1}):\n",
        "    sessions = x_train['session_id'].unique()\n",
        "    all_jobs = set()\n",
        "    for jobs in x_train['job_ids']:\n",
        "        all_jobs.update(jobs)\n",
        "    all_jobs = list(all_jobs)\n",
        "\n",
        "    session2idx = {s: i for i, s in enumerate(sessions)}\n",
        "    job2idx = {j: i for i, j in enumerate(all_jobs)}\n",
        "    # job2idx = {row[\"job_id\"]: idx for idx, row in x_train.iterrows()}\n",
        "\n",
        "    matrix = np.zeros((len(sessions), len(all_jobs)))\n",
        "    for _, row in x_train.iterrows():\n",
        "        s_idx = session2idx[row['session_id']]\n",
        "        for job, act in zip(row['job_ids'], row['actions']):\n",
        "            j_idx = job2idx[job]\n",
        "            matrix[s_idx, j_idx] += weight_map.get(act,1)\n",
        "\n",
        "    return matrix, session2idx, job2idx\n",
        "\n",
        "#interaction_matrix, session2idx, _ = build_interaction_matrix(df_x) # interaction_matrix.shape = (n_sessions , n_jobs)\n",
        "interaction_matrix, session2idx, job2idx = build_interaction_matrix(df_x) # interaction_matrix.shape = (n_sessions , n_jobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p90xCiOI0RlH"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Compute Session Features for Action Prediction from df_x\n",
        "# ---------------------------\n",
        "# (Assuming each row in df_x has a list of actions corresponding to the jobs in that session)\n",
        "df_x['apply_count'] = df_x['actions'].apply(lambda acts: sum(1 for act in acts if act == 'apply'))\n",
        "df_x['view_count'] = df_x['actions'].apply(lambda acts: sum(1 for act in acts if act == 'view'))\n",
        "df_x['total'] = df_x['apply_count'] + df_x['view_count']\n",
        "\n",
        "session_features = df_x.groupby('session_id').agg({\n",
        "    'total': 'sum',\n",
        "    'apply_count': 'sum',\n",
        "    'view_count': 'sum'\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNEJGrr00UQe"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 3. Align Job Embeddings (Using GPU)\n",
        "# ---------------------------\n",
        "# Create a mapping from job id (as string) to the row index in the job_embedding_matrix\n",
        "\n",
        "def build_aligned_embeddings(job2idx, job_embedding_matrix):\n",
        "\n",
        "\n",
        "    embedding_map = {str(idx): idx for idx in range(job_embedding_matrix.shape[0])}\n",
        "    num_jobs = len(job2idx)\n",
        "    emb_dim = job_embedding_matrix.shape[1]\n",
        "    aligned_embeddings = torch.zeros((num_jobs, emb_dim), device=device)\n",
        "\n",
        "    available_embeddings = []\n",
        "    for job, idx in job2idx.items():\n",
        "        job_str = str(job)\n",
        "        if job_str in embedding_map:\n",
        "            emb = job_embedding_matrix[embedding_map[job_str]]\n",
        "            aligned_embeddings[idx] = emb\n",
        "            available_embeddings.append(emb)\n",
        "\n",
        "    if available_embeddings:\n",
        "        avg_embedding = torch.stack(available_embeddings).mean(dim=0)\n",
        "        missing = torch.where(aligned_embeddings.norm(dim=1) == 0)[0]\n",
        "        aligned_embeddings[missing] = avg_embedding\n",
        "\n",
        "    return aligned_embeddings\n",
        "\n",
        "aligned_embeddings = build_aligned_embeddings(job2idx, job_embedding_matrix)\n",
        "#aligned_embeddings = build_aligned_embeddings(job2idx, job_emb_matrix_pca)\n",
        "#aligned_embeddings = job_embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLfJT7Wk0W0a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 4. Compute User Factors Using SVD (Collaborative Filtering)\n",
        "# ---------------------------\n",
        "n_components = 200\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "user_factors = svd.fit_transform(interaction_matrix)  # Shape: (num_sessions, n_components)\n",
        "job_factors = svd.components_.T  # Shape: (num_jobs, n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrKDLyo80avx"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 5. Hybrid Recommendation with GPU-Based Content Filtering\n",
        "# ---------------------------\n",
        "def hybrid_recommendation(session_idx, user_factors, job_factors, interaction_matrix, aligned_embeddings, alpha=0.7, top_n=10):\n",
        "    \"\"\"Hybrid CF + GPU-based CBF recommendation system.\"\"\"\n",
        "\n",
        "    # Get Collaborative Filtering (CF) Scores\n",
        "    cf_scores = user_factors[session_idx] @ job_factors.T  # Shape: (num_jobs,)\n",
        "    cf_scores = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min() + 1e-9)\n",
        "    #cf_scores = torch.full((aligned_embeddings.shape[0],), cf_scores.mean(), device=device)\n",
        "\n",
        "\n",
        "    # Build session embedding (weighted job embeddings) from the interaction matrix\n",
        "    interacted_jobs = np.where(interaction_matrix[session_idx] > 0)[0]  # Indices of jobs the user interacted with\n",
        "\n",
        "    if len(interacted_jobs) > 0:\n",
        "        session_embeddings = aligned_embeddings[interacted_jobs]  # Shape: (num_interacted_jobs, emb_dim)\n",
        "        action_weights = torch.tensor(interaction_matrix[session_idx, interacted_jobs], device=device).float()\n",
        "        action_weights = action_weights.view(-1, 1)\n",
        "        user_profile_vector = torch.sum(session_embeddings * action_weights, dim=0) / torch.sum(action_weights)\n",
        "    else:\n",
        "        user_profile_vector = torch.mean(aligned_embeddings, dim=0)  # Fallback if no interactions\n",
        "\n",
        "    # Compute Content-Based Filtering (CBF) Scores using cosine similarity on GPU\n",
        "    cbf_scores = F.cosine_similarity(user_profile_vector.unsqueeze(0), aligned_embeddings, dim=1)  # (num_jobs,)\n",
        "\n",
        "    # Final Hybrid Score: blend CF and CBF scores using alpha\n",
        "    final_scores = alpha * torch.tensor(cf_scores, device=device) + (1 - alpha) * cbf_scores\n",
        "\n",
        "    # Exclude already interacted jobs from recommendations\n",
        "    final_scores[torch.tensor(interaction_matrix[session_idx], device=device) > 0] = float('-inf')\n",
        "\n",
        "    # Get top-N recommended job indices\n",
        "    top_indices = torch.argsort(final_scores, descending=True)[:top_n].cpu().numpy()\n",
        "    return top_indices"
      ]
    },
    {
      "source": [
        "# ---------------------------\n",
        "# 5. Hybrid Recommendation with GPU-Based Content Filtering\n",
        "# ---------------------------\n",
        "def hybrid_recommendation(session_idx, user_factors, job_factors, interaction_matrix, aligned_embeddings, alpha=0.7, top_n=10):\n",
        "    \"\"\"Hybrid CF + GPU-based CBF recommendation system.\"\"\"\n",
        "\n",
        "    # Get Collaborative Filtering (CF) Scores\n",
        "    cf_scores = user_factors[session_idx] @ job_factors.T  # Shape: (num_jobs,)\n",
        "    cf_scores = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min() + 1e-9)\n",
        "    cf_scores = torch.full((aligned_embeddings.shape[0],), 0, device=device)\n",
        "\n",
        "\n",
        "\n",
        "    # Build session embedding (weighted job embeddings) from the interaction matrix\n",
        "    interacted_jobs = np.where(interaction_matrix[session_idx] > 0)[0]  # Indices of jobs the user interacted with\n",
        "\n",
        "    if len(interacted_jobs) > 0:\n",
        "        session_embeddings = aligned_embeddings[interacted_jobs]  # Shape: (num_interacted_jobs, emb_dim)\n",
        "        action_weights = torch.tensor(interaction_matrix[session_idx, interacted_jobs], device=device).float()\n",
        "        action_weights = action_weights.view(-1, 1)\n",
        "        user_profile_vector = torch.sum(session_embeddings * action_weights, dim=0) / torch.sum(action_weights)\n",
        "    else:\n",
        "        user_profile_vector = torch.mean(aligned_embeddings, dim=0)  # Fallback if no interactions\n",
        "\n",
        "    # Compute Content-Based Filtering (CBF) Scores using cosine similarity on GPU\n",
        "    cbf_scores = F.cosine_similarity(user_profile_vector.unsqueeze(0), aligned_embeddings, dim=1)  # (num_jobs,)\n",
        "\n",
        "    # Final Hybrid Score: blend CF and CBF scores using alpha\n",
        "    final_scores = alpha * cf_scores.clone().detach().float() + (1 - alpha) * cbf_scores\n",
        "\n",
        "    # Exclude already interacted jobs from recommendations\n",
        "    # Create a mask with the correct size for final_scores\n",
        "    mask = torch.zeros_like(final_scores, dtype=torch.bool, device=device)\n",
        "    # Map interacted job indices to the corresponding indices in final_scores\n",
        "    interacted_job_indices_in_final_scores = [job2idx[job_id] for job_id in df_x.loc[df_x['session_id'] == s_id, 'job_ids'].iloc[0] if job_id in job2idx]\n",
        "    mask[interacted_job_indices_in_final_scores] = True\n",
        "    final_scores[mask] = float('-inf')\n",
        "\n",
        "    # Get top-N recommended job indices\n",
        "    top_indices = torch.argsort(final_scores, descending=True)[:top_n].cpu().numpy()\n",
        "    return top_indices"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "n_jHQepg53lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUVq2xSYqfLX"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 6. Recommendation Pipeline & Evaluation (GPU-Based)\n",
        "# ---------------------------\n",
        "results = {}\n",
        "y_true_jobs = []\n",
        "y_pred_jobs = []\n",
        "y_true_actions = []\n",
        "y_pred_actions = []\n",
        "idx2job = {i: j for j, i in job2idx.items()}\n",
        "\n",
        "for _, row in df_y.iterrows():\n",
        "    s_id = row['session_id']\n",
        "    if s_id in session2idx:\n",
        "        s_idx = session2idx[s_id]\n",
        "\n",
        "        # Get top-N job recommendations for this session\n",
        "        top_indices = hybrid_recommendation(s_idx, user_factors, job_factors, interaction_matrix, aligned_embeddings, alpha=0.5, top_n=10)\n",
        "        recommended_jobs = [i for i in top_indices]\n",
        "\n",
        "        # Save the ground truth and predicted job recommendations\n",
        "        y_true_jobs.append(row['job_id'])\n",
        "        y_pred_jobs.append(recommended_jobs)\n",
        "\n",
        "        # Use session features from df_x to predict action (since df_y only contains one action per session)\n",
        "        feat = session_features[session_features['session_id'] == s_id][['total', 'apply_count', 'view_count']]\n",
        "        if not feat.empty:\n",
        "            predicted_action = 'apply' if feat['apply_count'].sum() > feat['view_count'].sum() else 'view'\n",
        "        else:\n",
        "            predicted_action = 'view'\n",
        "\n",
        "        y_true_actions.append(row['action'])\n",
        "        y_pred_actions.append(predicted_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlVVtevg0d-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ac9f81-9507-4ce8-9aac-7397ef32dcba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "  🔹 MRR Score: 0.0006\n",
            "  🔹 Action Prediction Accuracy: 0.7766\n",
            "  🔹 Final Score (70% MRR + 30% Accuracy): 0.2334\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---------------------------\n",
        "# 7. Evaluation Metrics\n",
        "# ---------------------------\n",
        "def mean_reciprocal_rank(y_true, y_pred):\n",
        "    scores = []\n",
        "    for true_job, ranked_jobs in zip(y_true, y_pred):\n",
        "        if true_job in ranked_jobs:\n",
        "            rank = ranked_jobs.index(true_job) + 1\n",
        "            scores.append(1 / rank)\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "mrr_score = mean_reciprocal_rank(y_true_jobs, y_pred_jobs)\n",
        "action_accuracy = accuracy_score(\n",
        "    [1 if x == 'apply' else 0 for x in y_true_actions],\n",
        "    [1 if x == 'apply' else 0 for x in y_pred_actions]\n",
        ")\n",
        "final_score = (0.7 * mrr_score) + (0.3 * action_accuracy)\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"  🔹 MRR Score: {mrr_score:.4f}\")\n",
        "print(f\"  🔹 Action Prediction Accuracy: {action_accuracy:.4f}\")\n",
        "print(f\"  🔹 Final Score (70% MRR + 30% Accuracy): {final_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N6U4dPm0CO5"
      },
      "source": [
        "## Second Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsscz3uG0geo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ1vKN0j0o27"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 3. Compute Job Popularity\n",
        "# ---------------------------\n",
        "job_popularity = np.sum(interaction_matrix > 0, axis=0)\n",
        "popularity_penalty = 1 / (job_popularity + 1)  # Inverse frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDyWAyAI0rIc"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 4. Compute User Factors Using SVD (Collaborative Filtering)\n",
        "# ---------------------------\n",
        "n_components = 200  # Increased from 100\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "user_factors = svd.fit_transform(interaction_matrix)\n",
        "job_factors = svd.components_.T\n",
        "\n",
        "# Normalize CF scores\n",
        "user_factors = (user_factors - user_factors.min()) / (user_factors.max() - user_factors.min() + 1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-60uhFH0uB8"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 5. Align Job Embeddings (Using GPU)\n",
        "# ---------------------------\n",
        "def build_aligned_embeddings(job2idx, job_embedding_matrix):\n",
        "    num_jobs = len(job2idx)\n",
        "    emb_dim = job_embedding_matrix.shape[1]\n",
        "    aligned_embeddings = torch.zeros((num_jobs, emb_dim), device=device)\n",
        "\n",
        "    for job, idx in job2idx.items():\n",
        "        if job in job_embeddings:\n",
        "            aligned_embeddings[idx] = job_embedding_matrix[job]\n",
        "\n",
        "    avg_embedding = aligned_embeddings.mean(dim=0)\n",
        "    missing = torch.where(aligned_embeddings.norm(dim=1) == 0)[0]\n",
        "    aligned_embeddings[missing] = avg_embedding\n",
        "\n",
        "    return aligned_embeddings\n",
        "\n",
        "# aligned_embeddings = build_aligned_embeddings(job2idx, job_embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjxY9brC0xCT"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 6. Hybrid Recommendation\n",
        "# ---------------------------\n",
        "def hybrid_recommendation(session_idx, user_factors, job_factors, interaction_matrix, aligned_embeddings, alpha=0.7, top_n=10):\n",
        "    \"\"\"Hybrid CF + GPU-based CBF recommendation system.\"\"\"\n",
        "\n",
        "    cf_scores = user_factors[session_idx] @ job_factors.T\n",
        "\n",
        "    interacted_jobs = np.where(interaction_matrix[session_idx] > 0)[0]\n",
        "    if len(interacted_jobs) > 0:\n",
        "        session_embeddings = aligned_embeddings[interacted_jobs]\n",
        "        action_weights = torch.tensor(interaction_matrix[session_idx, interacted_jobs], device=device).float()\n",
        "        action_weights = action_weights.view(-1, 1)\n",
        "        user_profile_vector = torch.sum(session_embeddings * action_weights, dim=0) / torch.sum(action_weights)\n",
        "    else:\n",
        "        user_profile_vector = aligned_embeddings.mean(dim=0)\n",
        "\n",
        "    cbf_scores = user_profile_vector @ aligned_embeddings.T\n",
        "\n",
        "    final_scores = alpha * torch.tensor(cf_scores, device=device) + (1 - alpha) * cbf_scores\n",
        "    final_scores *= torch.tensor(popularity_penalty, device=device)\n",
        "\n",
        "    final_scores[torch.tensor(interaction_matrix[session_idx], device=device) > 0] = float('-inf')\n",
        "\n",
        "    top_indices = torch.argsort(final_scores, descending=True)[:top_n].cpu().numpy()\n",
        "    return top_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvdjqhApqfLZ"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# MLP Model\n",
        "# ---------------------------\n",
        "class ActionPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Cross-Validation Setup\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "all_mrr_scores = []\n",
        "all_action_accuracies = []\n",
        "best_model = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(df_x)):\n",
        "    print(f\"\\n Fold {fold+1} of Cross-Validation\")\n",
        "\n",
        "    # Split original dataset\n",
        "    df_x_train, df_x_test = df_x.iloc[train_idx], df_x.iloc[test_idx]\n",
        "    df_y_train, df_y_test = df_y.iloc[train_idx], df_y.iloc[test_idx]\n",
        "\n",
        "    # Expand dataset\n",
        "    df_x_train_expanded, df_y_train_expanded = expand_dataset(df_x_train, df_y_train)\n",
        "\n",
        "    # Extract features and labels\n",
        "    X_train = df_x_train_expanded[['total', 'apply_count', 'view_count']].values\n",
        "    y_train = (df_x_train_expanded['apply_count'] > df_x_train_expanded['view_count']).astype(int).values\n",
        "    X_test = df_x_test[['total', 'apply_count', 'view_count']].values\n",
        "    y_test = (df_x_test['apply_count'] > df_x_test['view_count']).astype(int).values\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Initialize model, optimizer, loss function\n",
        "    model = ActionPredictor().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(60):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train).squeeze()\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate Action Prediction\n",
        "    y_pred_test = model(X_test).squeeze().round().detach().cpu().numpy()\n",
        "    action_accuracy = accuracy_score(y_test.cpu().numpy(), y_pred_test)\n",
        "\n",
        "    # Compute MRR\n",
        "    def mean_reciprocal_rank(y_true, y_pred):\n",
        "        scores = []\n",
        "        for true_job, ranked_jobs in zip(y_true, y_pred):\n",
        "            if true_job in ranked_jobs:\n",
        "                rank = ranked_jobs.index(true_job) + 1\n",
        "                scores.append(1 / rank)\n",
        "            else:\n",
        "                scores.append(0)\n",
        "        return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "    mrr_score = mean_reciprocal_rank(y_true_jobs, y_pred_jobs)\n",
        "    final_score = (0.7 * mrr_score) + (0.3 * action_accuracy)\n",
        "\n",
        "    all_mrr_scores.append(mrr_score)\n",
        "    all_action_accuracies.append(action_accuracy)\n",
        "\n",
        "    # Check if this is the best model\n",
        "    if final_score > best_score:\n",
        "        best_score = final_score\n",
        "        best_model = model  # Keep best model in memory\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"  🔹 Fold {fold+1} Results:\")\n",
        "    print(f\"     - MRR Score: {mrr_score:.4f}\")\n",
        "    print(f\"     - Action Accuracy: {action_accuracy:.4f}\")\n",
        "    print(f\"     - Final Score: {final_score:.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Final Cross-Validation Scores\n",
        "# ---------------------------\n",
        "avg_mrr = sum(all_mrr_scores) / len(all_mrr_scores)\n",
        "avg_action_acc = sum(all_action_accuracies) / len(all_action_accuracies)\n",
        "final_cv_score = (0.7 * avg_mrr) + (0.3 * avg_action_acc)\n",
        "\n",
        "print(\"\\n Final Cross-Validation Results:\")\n",
        "print(f\"  🔹 Average MRR Score: {avg_mrr:.4f}\")\n",
        "print(f\"  🔹 Average Action Accuracy: {avg_action_acc:.4f}\")\n",
        "print(f\"  🔹 Final Score (70% MRR + 30% Accuracy): {final_cv_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq1clyzZhCIN",
        "outputId": "d9e2aa80-87e6-412a-ee31-99234b6d0b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold 1 of Cross-Validation\n",
            "  🔹 Fold 1 Results:\n",
            "     - MRR Score: 0.0006\n",
            "     - Action Accuracy: 0.9723\n",
            "     - Final Score: 0.2921\n",
            "\n",
            " Fold 2 of Cross-Validation\n",
            "  🔹 Fold 2 Results:\n",
            "     - MRR Score: 0.0006\n",
            "     - Action Accuracy: 0.9846\n",
            "     - Final Score: 0.2958\n",
            "\n",
            " Fold 3 of Cross-Validation\n",
            "  🔹 Fold 3 Results:\n",
            "     - MRR Score: 0.0006\n",
            "     - Action Accuracy: 0.9959\n",
            "     - Final Score: 0.2992\n",
            "\n",
            " Fold 4 of Cross-Validation\n",
            "  🔹 Fold 4 Results:\n",
            "     - MRR Score: 0.0006\n",
            "     - Action Accuracy: 0.9537\n",
            "     - Final Score: 0.2865\n",
            "\n",
            " Fold 5 of Cross-Validation\n",
            "  🔹 Fold 5 Results:\n",
            "     - MRR Score: 0.0006\n",
            "     - Action Accuracy: 0.9185\n",
            "     - Final Score: 0.2759\n",
            "\n",
            " Final Cross-Validation Results:\n",
            "  🔹 Average MRR Score: 0.0006\n",
            "  🔹 Average Action Accuracy: 0.9650\n",
            "  🔹 Final Score (70% MRR + 30% Accuracy): 0.2899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation - Measuring Model Performance"
      ],
      "metadata": {
        "id": "7ub0fExOQTZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(X, y, n_folds=5):\n",
        "    mrr_validation_error = []\n",
        "    action_validation_error = []\n",
        "\n",
        "    # split dataset\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "        print(f\"Fold {fold+1}:\")\n",
        "\n",
        "        # Split the data\n",
        "        x_train, x_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        # Expanding window on the session dataset\n",
        "        x_train, y_train = expand_dataset(x_train, y_train)\n",
        "        x_val, y_val = expand_dataset(x_val, y_val)\n",
        "\n",
        "        # Run the model on (x_train, y_train)\n",
        "\n",
        "        # Evaluate the model on (x_val, y_val)\n",
        "        mrr = 0\n",
        "        action_accuracy = 0\n",
        "\n",
        "\n",
        "        # Save the Fold evaluation results\n",
        "        mrr_validation_error.append(mrr)\n",
        "        action_validation_error.append(action_accuracy)\n",
        "\n",
        "    print(mrr_validation_error)\n",
        "    print(action_validation_error)\n",
        "    return mrr_validation_error, action_validation_error\n",
        "\n",
        "# cross_validation(df_x, df_y)"
      ],
      "metadata": {
        "id": "rkDMOsRhPKwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Create a temporary ordering for all jobs using the keys from job_embeddings\n",
        "features_list = list(job_embeddings.keys())\n",
        "num_features = len(features_list)  # Number of features for which we have embeddings\n",
        "pca_dim = 50  # Choose number of PCA components\n",
        "\n",
        "# Apply PCA separately to each feature embedding\n",
        "reduced_embeddings = {}\n",
        "for feature in features_list:\n",
        "    pca_model = PCA(n_components=pca_dim)\n",
        "    feature_emb_np = job_embeddings[feature].cpu().numpy()\n",
        "    feature_emb_pca_np = pca_model.fit_transform(feature_emb_np)\n",
        "    reduced_embeddings[feature] = torch.tensor(feature_emb_pca_np, dtype=torch.float32, device=device)\n",
        "\n",
        "# Concatenate the reduced feature embeddings\n",
        "job_emb_matrix_pca = torch.cat([reduced_embeddings[f] for f in features_list], dim=1)\n",
        "# job_emb_matrix_pca: (num_jobs, pca_dim * num_features)"
      ],
      "metadata": {
        "id": "QUzsglob3dRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_embedding_matrix.shape, job_emb_matrix_pca.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8yUaVVE4LUK",
        "outputId": "a00eaa83-0a6d-4ed4-a324-41c4b8a3a7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21917, 5120]) torch.Size([21917, 250])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, output_dim=64): # add hidden_dim\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),  # input_dim is now 250 (from pca_dim * num_features)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim) # connecting hidden to output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Create the MLP model\n",
        "input_dim = job_emb_matrix_pca.shape[1] # matrix of shape (num_jobs, num_features*emb_dim_after_pca)\n",
        "\n",
        "model_mlp = MLP(input_dim=input_dim).to(device)"
      ],
      "metadata": {
        "id": "SVwj0j6K5kiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_x.loc[:,\"job_ids\"] = df_x[\"job_ids\"].apply(ast.literal_eval)  # e.g., [102, 203, ...]\n",
        "# df_x.loc[:,\"actions\"] = df_x[\"actions\"].apply(ast.literal_eval)  # e.g., [1, 2, ...]\n",
        "df_x.loc[:,\"job_ids\"] = df_x[\"job_ids\"].apply(lambda jobs: [int(j) for j in jobs])  # e.g., [102, 203, ...]\n",
        "df_x.loc[:,\"actions\"] = df_x[\"actions\"].apply(lambda actions: [action_weights[a] for a in actions])  # e.g., [1, 2, ...]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CsxGXJkeIKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "def train(model, x, y, job_emb_matrix_pca, num_epochs=50, lr=0.001, eval_every=10):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()  # Loss function to optimize ranking\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute transformed embeddings for all jobs using the MLP model\n",
        "        batch_size = 64  # Adjust this size to fit the GPU memory\n",
        "        transformed_embeddings = []\n",
        "        for i in range(0, job_emb_matrix_pca.shape[0], batch_size):\n",
        "            batch = job_emb_matrix_pca[i:i + batch_size].to(device).float()  # Move batch to GPU and ensure it's float32\n",
        "            with torch.no_grad():  # No need to compute gradients for inference\n",
        "                transformed_embeddings.append(model(batch))  # Pass batch through the model\n",
        "            del batch  # Explicitly delete to free up memory after use\n",
        "\n",
        "        # Concatenate all embeddings after batch processing\n",
        "        transformed_embeddings = torch.cat(transformed_embeddings, dim=0)  # Shape: (num_jobs, emb_dim)\n",
        "\n",
        "        torch.cuda.empty_cache()  # Clear GPU cache\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        # Loop over each session in x\n",
        "        for i in range(len(x)):\n",
        "            session_job_ids = x.iloc[i][\"job_ids\"]   # e.g., [101, 205, 309, ...]\n",
        "            session_actions = torch.tensor(x.iloc[i][\"actions\"], dtype=torch.float32, device=device)\n",
        "\n",
        "            session_embeddings = transformed_embeddings[session_job_ids]  # shape: (num_jobs_in_session, emb_dim)\n",
        "            weights = session_actions.view(-1, 1)  # Reshape to match (num_jobs, 1)\n",
        "            session_profile = (session_embeddings * weights).sum(dim=0) / weights.sum()  # Weighted mean\n",
        "            session_profile = session_profile.requires_grad_()\n",
        "\n",
        "            # Cosine similarity\n",
        "            batch_size_similarity = 128  # Adjust batch size based on GPU memory\n",
        "            similarities = torch.zeros(len(transformed_embeddings), device=device)\n",
        "            with torch.no_grad():\n",
        "                for j in range(0, len(transformed_embeddings), batch_size_similarity):\n",
        "                    batch = transformed_embeddings[j:j + batch_size_similarity]\n",
        "                    similarities[j:j + batch_size_similarity] = F.cosine_similarity(session_profile.unsqueeze(0), batch)\n",
        "\n",
        "            # Rank the jobs by similarity\n",
        "            similarities = similarities.requires_grad_()\n",
        "\n",
        "            ranked_jobs = torch.argsort(similarities, descending=True)\n",
        "\n",
        "            # Compute loss - Encourage true job to rank high\n",
        "            y_true = int(y.iloc[i][\"job_id\"])\n",
        "            y_true = torch.tensor([y_true], dtype=torch.long, device=device)  # Convert job ID to index\n",
        "\n",
        "            # CrossEntropyLoss requires the predictions to be raw scores (logits), not probabilities\n",
        "            loss = loss_fn(similarities.unsqueeze(0), y_true)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            del session_embeddings  # Delete session_embeddings after use\n",
        "            del session_actions     # Delete session_actions after use\n",
        "            torch.cuda.empty_cache()  # Clear GPU cache after each session\n",
        "\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(x):.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "znqLw9qh_btq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(df_x, df_y)):\n",
        "    print(f\"Fold {fold+1}:\")\n",
        "\n",
        "    # Split the data\n",
        "    x_train, x_val = df_x.iloc[train_index], df_x.iloc[val_index]\n",
        "    y_train, y_val = df_y.iloc[train_index], df_y.iloc[val_index]\n",
        "\n",
        "    train(model_mlp, x_train, y_train, job_emb_matrix_pca, num_epochs=50, lr=0.001, eval_every=10)\n",
        "\n",
        "    # y_pred = model(x_val)\n",
        "\n",
        "    # mrr_score = mean_reciprocal_rank(y_true_jobs, y_pred_jobs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Is1YDdM2_rHn",
        "outputId": "32641417-740d-4e5a-a9b2-5f57c0a3a7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-25b6c434e0c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_emb_matrix_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# y_pred = model(x_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-7cbda469a620>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y, job_emb_matrix_pca, num_epochs, lr, eval_every)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_similarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_similarity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_similarity\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Rank the jobs by similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training loop for the MLP model using the CBF score as supervision\n",
        "\n",
        "# def train(model, x, y, num_epochs=50, lr=0.001, eval_every=10):\n",
        "#     model.train()\n",
        "\n",
        "#     optimizer_mlp = optim.Adam(model_mlp.parameters(), lr=0.001)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         optimizer_mlp.zero_grad()\n",
        "\n",
        "#         # Compute transformed embeddings for all jobs using the current MLP model\n",
        "#         transformed_embeddings = model_mlp(job_emb_matrix_pca)  # shape: (num_jobs, raw_emb_dim)\n",
        "#         aligned_embeddings = build_aligned_embeddings(job2idx, transformed_embeddings)\n",
        "\n",
        "#         # Loop over each session in x\n",
        "#         for i in range(len(x)):\n",
        "#             # Retrieve the session's job_ids and actions\n",
        "#             session_job_ids = x.iloc[i][\"job_ids\"]     # e.g., [101, 205, 309, ...]\n",
        "#             session_actions = x.iloc[i][\"actions\"]       # e.g., [1, 2, 1, ...]\n",
        "\n",
        "#             # Build a session profile as a weighted average of the job embeddings\n",
        "#             session_profile = torch.zeros(raw_emb_dim, device=device)\n",
        "#             weight_sum = 0.0\n",
        "#             for job, action in zip(session_job_ids, session_actions):\n",
        "#                 if job in temp_job2idx:\n",
        "#                     idx = temp_job2idx[job]\n",
        "#                     session_profile += transformed_embeddings[idx] * action\n",
        "#                     weight_sum += action\n",
        "#             if weight_sum > 0:\n",
        "#                 session_profile /= weight_sum\n",
        "#             else:\n",
        "#                 session_profile = transformed_embeddings.mean(dim=0)\n",
        "\n",
        "#             # Compute CBF scores as dot products between the session profile and all job embeddings\n",
        "#             scores = torch.matmul(transformed_embeddings, session_profile)  # shape: (num_jobs,)\n",
        "\n",
        "#             # Retrieve the target job for the session from df_y (assumed column \"target_job\")\n",
        "#             target_job = y.iloc[i][\"job_id\"]\n",
        "#             if target_job not in temp_job2idx:\n",
        "#                 continue  # Skip if target job not found\n",
        "#             target_idx = temp_job2idx[target_job]\n",
        "\n",
        "#             # Prepare scores and target for cross-entropy loss (add batch dimension)\n",
        "#             scores = scores.unsqueeze(0)  # now shape: (1, num_jobs)\n",
        "#             target_tensor = torch.tensor([target_idx], dtype=torch.long, device=device)\n",
        "\n",
        "#             loss = criterion(scores, target_tensor)\n",
        "#             total_loss += loss\n",
        "#             total_loss = total_loss / valid_sessions\n",
        "#             total_loss.backward()\n",
        "#             optimizer_mlp.step()\n",
        "#             print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# # After training, update the job embedding matrix (to be used later in the pipeline)\n",
        "# model_mlp.eval()\n",
        "# with torch.no_grad():\n",
        "#     job_embedding_matrix = model_mlp(job_emb_matrix_pca)\n",
        "# print(\"MLP training complete. New job_embedding_matrix shape:\", job_embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "Djc3a5Wk6jz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "y_true_jobs = []\n",
        "y_pred_jobs = []\n",
        "y_true_actions = []\n",
        "y_pred_actions = []\n",
        "idx2job = {i: j for j, i in job2idx.items()}\n",
        "\n",
        "for _, row in df_y.iterrows():\n",
        "    s_id = row['session_id']\n",
        "    if s_id in session2idx:\n",
        "        s_idx = session2idx[s_id]\n",
        "\n",
        "        interacted_jobs = np.where(interaction_matrix[s_idx] > 0)[0]\n",
        "        if len(interacted_jobs) > 0:\n",
        "            session_embeddings = aligned_embeddings[interacted_jobs]\n",
        "            action_weights = torch.tensor(interaction_matrix[s_idx, interacted_jobs], device=device).float()\n",
        "            action_weights = action_weights.view(-1, 1)\n",
        "            user_profile_vector = torch.sum(session_embeddings * action_weights, dim=0) / torch.sum(action_weights)\n",
        "        else:\n",
        "            user_profile_vector = aligned_embeddings.mean(dim=0)\n",
        "\n",
        "        cbf_scores = user_profile_vector @ aligned_embeddings.T"
      ],
      "metadata": {
        "id": "REqh27eA-rsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsYIFmFFNrTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result test:"
      ],
      "metadata": {
        "id": "8ujntp-zra5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "df_challenge = pd.read_csv('/content/x_test_jCBBNP2.csv')\n",
        "df_challenge[\"job_ids\"] = df_challenge[\"job_ids\"].apply(ast.literal_eval)\n",
        "df_challenge[\"actions\"] = df_challenge[\"actions\"].apply(ast.literal_eval)\n",
        "\n",
        "# (Assuming each row in df_x has a list of actions corresponding to the jobs in that session)\n",
        "df_challenge['apply_count'] = df_challenge['actions'].apply(lambda acts: sum(1 for act in acts if act == 'apply'))\n",
        "df_challenge['view_count'] = df_challenge['actions'].apply(lambda acts: sum(1 for act in acts if act == 'view'))\n",
        "df_challenge['total'] = df_challenge['apply_count'] + df_challenge['view_count']\n",
        "\n",
        "# ---------------------------\n",
        "# Run Predictions on x_challenge\n",
        "# ---------------------------\n",
        "results = []\n",
        "idx2job = {i: j for j, i in job2idx.items()}\n",
        "\n",
        "for _, row in df_challenge.iterrows():\n",
        "    s_id = row['session_id']\n",
        "\n",
        "    if s_id in session2idx:\n",
        "        s_idx = session2idx[s_id]\n",
        "\n",
        "        # Get top-10 job recommendations\n",
        "        top_indices = hybrid_recommendation(s_idx, user_factors, job_factors, interaction_matrix, aligned_embeddings, alpha=0.75, top_n=10)\n",
        "        recommended_jobs = [idx2job[i] for i in top_indices]\n",
        "    else:\n",
        "        recommended_jobs = []  # No interaction history, fallback to empty\n",
        "\n",
        "    # Predict action using trained MLP\n",
        "    feat = row[['total', 'apply_count', 'view_count']].astype(float).values.reshape(1, -1)\n",
        "    feat = torch.tensor(feat, dtype=torch.float32, device=device)\n",
        "\n",
        "    predicted_action = 'apply' if best_model(feat).item() > 0.5 else 'view'\n",
        "\n",
        "    # Store result\n",
        "    results.append([s_id, predicted_action, str(recommended_jobs)])\n",
        "\n",
        "# ---------------------------\n",
        "# Save Results to CSV\n",
        "# ---------------------------\n",
        "df_results = pd.DataFrame(results, columns=['session_id', 'action', 'job_id'])\n",
        "df_results.to_csv('y_challenge_mlp.csv', index=False)"
      ],
      "metadata": {
        "id": "qmHLVI0Qrdw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6788457,
          "sourceId": 10919428,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6826704,
          "sourceId": 10971293,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6831793,
          "sourceId": 10978498,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}